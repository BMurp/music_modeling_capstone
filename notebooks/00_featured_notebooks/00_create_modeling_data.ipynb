{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data For Modeling\n",
    "This is the working notebook used to create the data used for modeling.\n",
    "\n",
    "As the capstone project was iterative, we created a few versions of data to expirement on different things.  This notebook has kept that last passes of data creation that were used in the analysis as a record and as an example. \n",
    "\n",
    "It can also be a reference for those who may want to create their own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "#the projects libary houses the code that does most of the heavy lifting \n",
    "#the notebook api's data loader provides the interface for combined source data, \n",
    "# as well as Modeling data, which is populated by the output of this prcess \n",
    "from library.notebook_api.data_loader import CombinedDataLoader, ModelDataLoader\n",
    "from library.source_data.parallel_processor import AudioParallelProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Extraction - For V006 data population\n",
    "This scenario runs extraction on a subset of the large version of the data set, filtering to rows matching a list of provided genres, and also trying to balance the dataset through obtaining an equal sized sample from each genre.   This is because data generation takes a while,  so doing it in batches and chunks means we have some data to work with on modeling tasks while we continue to batch create more data\n",
    "\n",
    "CombinedDataLoader is the primary class used to interact with the source data, the first argument is for which FMA data source to use and it supports large, medium, and small.  small is the only one that comes in our project_data_source folder as it is impracticle for us to store large and medium given their size.  \n",
    "See the Readme for more details on data, for CombinedDataLoader to work it requires that the project_data_source folder is populated with the source dataset assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracks in meta 29701\n",
      "tracks with files available in project_data_path:  29701\n",
      "tracks with top level genres available 29701\n",
      "tracks with genres and files (df_filtered) 29701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15947"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantate data_loader and the dataframes it makes available \n",
    "in_scope_labels = ['rock', 'electronic', 'hiphop', 'classical', 'jazz','country']\n",
    "data_loader = CombinedDataLoader('large', in_scope_labels)\n",
    "\n",
    "#this data loader method, samples the soruce data separately for each provided genre in the in_scope_labels list\n",
    "#the argument controls the number of samples per genre\n",
    "#if you run out of trakcs per genre, it will just stop at the max track \n",
    "df_balanced = data_loader.get_data_sampled_by_label(5000)\n",
    "\n",
    "len(df_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is currently setup to update using incremental batches.\n",
    "The code below checks the current context of the Model data version and determines the list of tracks that have yet to been process from the source\n",
    "\n",
    "This only really makes sense if you have already populated data into a version\n",
    "If it is to be resed you can skip that part and just pass df_balanced, or CombinedDataLoader.df_filtered to the audio processor in teh next step\n",
    "\n",
    "Note - some of the files had errors when trying to process, in which case they are skipped.  we consider it out of the scope of the project to investigate and address the errors, but it can be a reason why a \"complete\" process does not have all of the tracks in the metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3494 files left to process \n"
     ]
    }
   ],
   "source": [
    "#get dataframe of current existing track ids from ModelDataLoader\n",
    "current_data = ModelDataLoader('006')\n",
    "current_data_df = current_data.df\n",
    "current_track_list = current_data_df[['track_id','file_available']].copy()\n",
    "#rename file_available column to differentiate it \n",
    "current_track_list.rename(columns={'file_available':'file_available_in_input'}, inplace=True)\n",
    "#join this to medium source data to figure out tracks we don't have \n",
    "df_filtered_with_current_track_list = pd.merge(df_balanced,current_track_list,on='track_id',how='left')\n",
    "#boolean indexer for the incremental files\n",
    "incremental = (df_filtered_with_current_track_list['file_available_in_input'].isnull()) &(df_filtered_with_current_track_list['file_available'] ==1)\n",
    "\n",
    "df_filtered_incremental = df_filtered_with_current_track_list[incremental].copy()\n",
    "\n",
    "print(len(df_filtered_incremental), 'files left to process ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we kept a record of the batches that were run. \n",
    "As you can see the AudioParallelProcessor takes arguments for version number, batch, threads\n",
    "These all become part of the file names and paths generated in `project_data_source/model_input_data`\n",
    "\n",
    "Each batch will also spawn multiple threads, according to the thread argument to allow to parallelize processing across multi-core cpus. \n",
    "\n",
    "SAMPLE_RATE and second configs inform the audio extraction using librosa, for this version we downsamples and truncated to reduce vector size and have consistent vector lengths. \n",
    "\n",
    "Note this version is the only one where both MFCCs and Log Melspectrograms are generated as a result of running the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE  = 22500\n",
    "SECONDS = 25\n",
    "#these batches include mfccs and log_melspectrogram, downsampled to 22500 , truncated to 25 seconds \n",
    "#batch_mfcc = AudioParallelProcessor(df_filtered_incremental.iloc[0:1200],version = '006', batch=2, threads = 4,sample_rate=SAMPLE_RATE,start_sample=0,end_sample=SAMPLE_RATE*SECONDS)\n",
    "#batch_mfcc2 = AudioParallelProcessor(df_filtered_incremental.iloc[1201:],version = '006', batch=3, threads = 4,sample_rate=SAMPLE_RATE,start_sample=0,end_sample=SAMPLE_RATE*SECONDS)\n",
    "#batch_mfcc = AudioParallelProcessor(df_filtered_incremental.iloc[0:2000],version = '006', batch=4, threads = 5,sample_rate=SAMPLE_RATE,start_sample=0,end_sample=SAMPLE_RATE*SECONDS)\n",
    "#batch_mfcc2 = AudioParallelProcessor(df_filtered_incremental.iloc[2001:4000],version = '006', batch=5, threads = 5,sample_rate=SAMPLE_RATE,start_sample=0,end_sample=SAMPLE_RATE*SECONDS)\n",
    "\n",
    "#batch_mfcc = AudioParallelProcessor(df_filtered_incremental.iloc[0:2000],version = '006', batch=6, threads = 5,sample_rate=SAMPLE_RATE,start_sample=0,end_sample=SAMPLE_RATE*SECONDS)\n",
    "#batch_mfcc2 = AudioParallelProcessor(df_filtered_incremental.iloc[2001:4000],version = '006', batch=7, threads = 5,sample_rate=SAMPLE_RATE,start_sample=0,end_sample=SAMPLE_RATE*SECONDS)\n",
    "\n",
    "#batch_mfcc = AudioParallelProcessor(df_filtered_incremental.iloc[0:2000],version = '006', batch=8, threads = 5,sample_rate=SAMPLE_RATE,start_sample=0,end_sample=SAMPLE_RATE*SECONDS)\n",
    "#batch_mfcc2 = AudioParallelProcessor(df_filtered_incremental.iloc[2001:4000],version = '006', batch=9, threads = 5,sample_rate=SAMPLE_RATE,start_sample=0,end_sample=SAMPLE_RATE*SECONDS)\n",
    "\n",
    "#batch_mfcc = AudioParallelProcessor(df_filtered_incremental.iloc[0:2000],version = '006', batch=10, threads = 5,sample_rate=SAMPLE_RATE,start_sample=0,end_sample=SAMPLE_RATE*SECONDS)\n",
    "#batch_mfcc2 = AudioParallelProcessor(df_filtered_incremental.iloc[2001:4000],version = '006', batch=11, threads = 5,sample_rate=SAMPLE_RATE,start_sample=0,end_sample=SAMPLE_RATE*SECONDS)\n",
    "\n",
    "batch_mfcc = AudioParallelProcessor(df_filtered_incremental,version = '006', batch=12, threads = 5,sample_rate=SAMPLE_RATE,start_sample=0,end_sample=SAMPLE_RATE*SECONDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_mfcc.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_mfcc2.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction fma_large V005 data population\n",
    "\n",
    "we completed this version after we downloaded and incororated fma_large data, but before we decided to narrow down to a subset of generes, and before we added Log melspectrogram vectors. \n",
    "\n",
    "As a result this version has repesentation for all tracks and has metadata, numerical features and MFCC vectors at full sample rate and length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Metadata\n",
    "pass the optional fma_audio size argument to Combined data loader to reference the fma_large file directory as part of combinding fma data with gtzan data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracks in meta 107574\n",
      "tracks with files available in project_data_path:  107574\n",
      "tracks with top level genres available 50598\n",
      "tracks with genres and files (df_filtered) 50598\n",
      "25720 files left to process \n"
     ]
    }
   ],
   "source": [
    "#instantate data_loader and the dataframes it makes available \n",
    "data_loader = CombinedDataLoader('large')\n",
    "df = data_loader.df\n",
    "df_files_available = data_loader.df_files_available\n",
    "df_genres_available = data_loader.df_genres_available\n",
    "df_filtered = data_loader.df_filtered\n",
    "\n",
    "#get dataframe of current existing track ids from ModelDataLoader\n",
    "current_data = ModelDataLoader('005')\n",
    "current_data_df = current_data.df\n",
    "current_track_list = current_data_df[['track_id','file_available']].copy()\n",
    "#rename file_available column to differentiate it \n",
    "current_track_list.rename(columns={'file_available':'file_available_in_input'}, inplace=True)\n",
    "#join this to medium source data to figure out tracks we don't have \n",
    "df_filtered_with_current_track_list = pd.merge(df_filtered,current_track_list,on='track_id',how='left')\n",
    "#boolean indexer for the incremental files\n",
    "incremental = (df_filtered_with_current_track_list['file_available_in_input'].isnull()) &(df_filtered_with_current_track_list['file_available'] ==1)\n",
    "\n",
    "df_filtered_incremental = df_filtered_with_current_track_list[incremental].copy()\n",
    "\n",
    "print(len(df_filtered_incremental), 'files left to process ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "hiphop       2173\n",
       "classical     444\n",
       "jazz          255\n",
       "country       202\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_scope_labels = ['rock', 'electronic', 'hiphop', 'classical', 'jazz','country']\n",
    "\n",
    "df_incremental_under_represented = df_filtered_incremental[df_filtered_incremental['label'].apply(lambda label: True if label in in_scope_labels else False)]\n",
    "df_incremental_under_represented['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and save numerical features in parquet and mfcc as npy nd array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_mfcc = AudioParallelProcessor(df_incremental_under_represented,version = '005', batch=12, threads = 4)\n",
    "\n",
    "#batch_mfcc = AudioParallelProcessor(df_filtered_incremental.iloc[0:2000],version = '005', batch=11, threads = 4)\n",
    "#batch_mfcc2 = AudioParallelProcessor(df_filtered_incremental.iloc[2001:4000],version = '005', batch=12, threads = 4)\n",
    "#batch_mfcc3 = AudioParallelProcessor(df_filtered_incremental.iloc[4001:6000],version = '005', batch=13, threads = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_mfcc.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_mfcc2.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_mfcc3.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ModelDataLoader('005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24878"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.df.head()\n",
    "len(m.df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
