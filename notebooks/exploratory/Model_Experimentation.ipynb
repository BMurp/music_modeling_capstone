{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-QJiMjJQ1A3"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-magic -q"
      ],
      "metadata": {
        "id": "zf81JEGEfwDk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r310dZFAKV0t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import warnings\n",
        "import magic\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import VotingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJCuGJ_YQ4Ub"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48MZ-xLjKZJ-",
        "outputId": "8a940604-77d7-4b6d-887b-4db7cf3b8a2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EviRLL7VMrom",
        "outputId": "7a6895da-33fb-4041-b41c-e5c52ac806df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File type: Apache Parquet\n"
          ]
        }
      ],
      "source": [
        "# Use the magic library to identify the file type\n",
        "file_path = \"/content/drive/MyDrive/model_input_002\"\n",
        "file_type = magic.from_file(file_path)\n",
        "\n",
        "print(f\"File type: {file_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEhD9ro4MvIA",
        "outputId": "0ccf6ffa-1029-43e9-89b3-edcf313b008a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  track_id dataset                                         audio_path   label  \\\n",
            "0        2     fma  /project_data_source/free_music_archive/fma_sm...  hiphop   \n",
            "1        5     fma  /project_data_source/free_music_archive/fma_sm...  hiphop   \n",
            "2       10     fma  /project_data_source/free_music_archive/fma_sm...     pop   \n",
            "3      140     fma  /project_data_source/free_music_archive/fma_sm...    folk   \n",
            "4      141     fma  /project_data_source/free_music_archive/fma_sm...    folk   \n",
            "\n",
            "  fma_genre_top fma_genres fma_genres_all  sampling_rate  \\\n",
            "0       Hip-Hop       [21]           [21]        44100.0   \n",
            "1       Hip-Hop       [21]           [21]        44100.0   \n",
            "2           Pop       [10]           [10]        44100.0   \n",
            "3          Folk       [17]           [17]        44100.0   \n",
            "4          Folk       [17]           [17]        44100.0   \n",
            "\n",
            "                                            features  \n",
            "0  [3683.9976, 1.2325847, -0.45687148, 3495.6704,...  \n",
            "1  [2928.3987, 0.6809109, 0.022613911, 3383.929, ...  \n",
            "2  [2786.7935, -0.8545609, -0.07638783, 2895.759,...  \n",
            "3  [1957.3481, 1.3831363, 0.35159805, 2927.9644, ...  \n",
            "4  [1768.6066, 1.122301, -0.099131316, 2358.3088,...  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 8996 entries, 0 to 8995\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   track_id        8996 non-null   string \n",
            " 1   dataset         8996 non-null   object \n",
            " 2   audio_path      8996 non-null   object \n",
            " 3   label           8996 non-null   object \n",
            " 4   fma_genre_top   8996 non-null   object \n",
            " 5   fma_genres      8996 non-null   string \n",
            " 6   fma_genres_all  8996 non-null   string \n",
            " 7   sampling_rate   8996 non-null   float64\n",
            " 8   features        8996 non-null   object \n",
            "dtypes: float64(1), object(5), string(3)\n",
            "memory usage: 702.8+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Load the parquet file\n",
        "df = pd.read_parquet(file_path, engine='pyarrow')\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Display basic information\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyusjxN3Q78y"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GridSearch with Best Parameters for Random Forest and XGBoost\n"
      ],
      "metadata": {
        "id": "5H3lN_VxpwE7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zzIjhNlyDun",
        "outputId": "8d58278f-8f80-42b9-cb65-adeb9de42743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Labels: ['blues' 'classical' 'country' 'electronic' 'experimental' 'folk' 'hiphop'\n",
            " 'instrumental' 'international' 'jazz' 'pop' 'rock' 'soulrnb']\n",
            "Training set size: (7196, 15)\n",
            "Test set size: (1800, 15)\n",
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
            "Best Random Forest Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "Best XGBoost Parameters: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 300, 'subsample': 0.8}\n",
            "\n",
            "Best Random Forest Accuracy: 0.515\n",
            "Classification Report for Random Forest:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "        blues       0.60      0.94      0.73        16\n",
            "    classical       0.73      0.89      0.80        18\n",
            "      country       0.40      0.70      0.51        23\n",
            "   electronic       0.50      0.44      0.47       193\n",
            " experimental       0.47      0.38      0.42       206\n",
            "         folk       0.52      0.59      0.55       196\n",
            "       hiphop       0.61      0.62      0.61       249\n",
            " instrumental       0.52      0.51      0.52       200\n",
            "international       0.50      0.52      0.51       228\n",
            "         jazz       0.50      0.62      0.56        16\n",
            "          pop       0.39      0.32      0.35       198\n",
            "         rock       0.57      0.60      0.58       242\n",
            "      soulrnb       0.29      0.53      0.37        15\n",
            "\n",
            "     accuracy                           0.52      1800\n",
            "    macro avg       0.51      0.59      0.54      1800\n",
            " weighted avg       0.51      0.52      0.51      1800\n",
            "\n",
            "\n",
            "Best XGBoost Accuracy: 0.5161111111111111\n",
            "Classification Report for XGBoost:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "        blues       0.58      0.88      0.70        16\n",
            "    classical       0.83      0.83      0.83        18\n",
            "      country       0.44      0.61      0.51        23\n",
            "   electronic       0.54      0.48      0.51       193\n",
            " experimental       0.45      0.38      0.41       206\n",
            "         folk       0.53      0.60      0.56       196\n",
            "       hiphop       0.67      0.65      0.66       249\n",
            " instrumental       0.48      0.47      0.48       200\n",
            "international       0.50      0.52      0.51       228\n",
            "         jazz       0.52      0.69      0.59        16\n",
            "          pop       0.30      0.31      0.30       198\n",
            "         rock       0.59      0.59      0.59       242\n",
            "      soulrnb       0.45      0.67      0.54        15\n",
            "\n",
            "     accuracy                           0.52      1800\n",
            "    macro avg       0.53      0.59      0.55      1800\n",
            " weighted avg       0.52      0.52      0.51      1800\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Safely evaluate the 'features' column\n",
        "df['features'] = df['features'].apply(lambda x: np.array(ast.literal_eval(x)) if isinstance(x, str) else x)\n",
        "\n",
        "# Step 2: Drop rows with missing features\n",
        "df = df.dropna(subset=['features'])\n",
        "\n",
        "# Step 3: Prepare features and labels\n",
        "X = np.stack(df['features'].values)  # Convert to numpy matrix\n",
        "y = df['label'].values               # Extract labels\n",
        "\n",
        "# Step 4: Encode labels into numerical values\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)  # Convert genres to numeric values\n",
        "\n",
        "# Check the label encoding\n",
        "print(f\"Encoded Labels: {le.classes_}\")\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "\n",
        "# Step 6: Apply SMOTE to balance the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Step 7: Define the parameter grids for Grid Search\n",
        "\n",
        "# Random Forest parameter grid\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# XGBoost parameter grid\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Step 8: Perform Grid Search on Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_grid_search = GridSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_grid=rf_param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "rf_grid_search.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Get the best Random Forest model and parameters\n",
        "best_rf = rf_grid_search.best_estimator_\n",
        "print(\"Best Random Forest Parameters:\", rf_grid_search.best_params_)\n",
        "\n",
        "# Step 9: Perform Grid Search on XGBoost\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "xgb_grid_search = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=xgb_param_grid,\n",
        "    cv=3,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "xgb_grid_search.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Get the best XGBoost model and parameters\n",
        "best_xgb = xgb_grid_search.best_estimator_\n",
        "print(\"Best XGBoost Parameters:\", xgb_grid_search.best_params_)\n",
        "\n",
        "# Step 10: Evaluate both models on the test set\n",
        "\n",
        "# Random Forest evaluation\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"\\nBest Random Forest Accuracy:\", accuracy_rf)\n",
        "print(\"Classification Report for Random Forest:\\n\", classification_report(y_test, y_pred_rf, target_names=le.classes_))\n",
        "\n",
        "# XGBoost evaluation\n",
        "y_pred_xgb = best_xgb.predict(X_test)\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(\"\\nBest XGBoost Accuracy:\", accuracy_xgb)\n",
        "print(\"Classification Report for XGBoost:\\n\", classification_report(y_test, y_pred_xgb, target_names=le.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlolkzGqngcl"
      },
      "source": [
        "### Weighted Soft Voting Ensemble with SMOTE for Improved Classification Performance (Random Forest and XGBoost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIAEJdWTnbFS",
        "outputId": "6493558a-932c-4168-c185-3c1fdae118ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Optimized Soft Voting Ensemble (RF + XGBoost): 0.5244444444444445\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "        blues       0.54      0.88      0.67        16\n",
            "    classical       0.75      0.83      0.79        18\n",
            "      country       0.48      0.70      0.57        23\n",
            "   electronic       0.53      0.45      0.49       193\n",
            " experimental       0.45      0.39      0.42       206\n",
            "         folk       0.52      0.58      0.55       196\n",
            "       hiphop       0.68      0.67      0.68       249\n",
            " instrumental       0.55      0.51      0.53       200\n",
            "international       0.50      0.53      0.52       228\n",
            "         jazz       0.55      0.75      0.63        16\n",
            "          pop       0.33      0.34      0.34       198\n",
            "         rock       0.59      0.56      0.58       242\n",
            "      soulrnb       0.38      0.67      0.49        15\n",
            "\n",
            "     accuracy                           0.52      1800\n",
            "    macro avg       0.53      0.61      0.56      1800\n",
            " weighted avg       0.53      0.52      0.52      1800\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Safely evaluate the 'features' column\n",
        "df['features'] = df['features'].apply(lambda x: np.array(ast.literal_eval(x)) if isinstance(x, str) else x)\n",
        "\n",
        "# Step 2: Drop rows with missing features\n",
        "df = df.dropna(subset=['features'])\n",
        "\n",
        "# Step 3: Prepare features and labels\n",
        "X = np.stack(df['features'].values)  # Convert to numpy matrix\n",
        "y = df['label'].values               # Extract labels\n",
        "\n",
        "# Step 4: Encode labels into numerical values\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)  # Convert genres to numeric values\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Apply SMOTE to balance the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Step 8: Define models with the best parameters\n",
        "best_rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,  # No limit on tree depth\n",
        "    min_samples_split=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "best_xgb = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.2,\n",
        "    subsample=0.8,\n",
        "    eval_metric='mlogloss',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 9: Create the weighted soft voting ensemble\n",
        "ensemble_model_weighted = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', best_rf),   # Random Forest\n",
        "        ('xgb', best_xgb)  # XGBoost\n",
        "    ],\n",
        "    voting='soft',\n",
        "    weights=[2, 3]  # Favor XGBoost due to higher performance\n",
        ")\n",
        "\n",
        "# Step 10: Train the ensemble model\n",
        "ensemble_model_weighted.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Step 11: Make predictions on the test data\n",
        "y_pred_weighted = ensemble_model_weighted.predict(X_test_scaled)\n",
        "\n",
        "# Step 12: Evaluate the model's performance\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "classification_report_weighted = classification_report(y_test, y_pred_weighted, target_names=le.classes_)\n",
        "\n",
        "# Output the results\n",
        "print(\"Accuracy with Optimized Soft Voting Ensemble (RF + XGBoost):\", accuracy_weighted)\n",
        "print(\"Classification Report:\\n\", classification_report_weighted)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted Soft Voting Ensemble with SMOTE for Improved Classification Performance (Random Forest, XGBoost, KNN, SVM)"
      ],
      "metadata": {
        "id": "imEjdPglqBc8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-xugi1-_CLV",
        "outputId": "a8b55d91-506b-4231-9c50-8e9102a386ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy with Extended Soft Voting Ensemble: 0.5361111111111111\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "        blues       0.60      0.94      0.73        16\n",
            "    classical       0.71      0.83      0.77        18\n",
            "      country       0.48      0.65      0.56        23\n",
            "   electronic       0.56      0.47      0.51       193\n",
            " experimental       0.47      0.40      0.43       206\n",
            "         folk       0.53      0.60      0.56       196\n",
            "       hiphop       0.69      0.69      0.69       249\n",
            " instrumental       0.55      0.53      0.54       200\n",
            "international       0.51      0.55      0.53       228\n",
            "         jazz       0.57      0.81      0.67        16\n",
            "          pop       0.34      0.32      0.33       198\n",
            "         rock       0.58      0.57      0.58       242\n",
            "      soulrnb       0.44      0.73      0.55        15\n",
            "\n",
            "     accuracy                           0.54      1800\n",
            "    macro avg       0.54      0.62      0.57      1800\n",
            " weighted avg       0.53      0.54      0.53      1800\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Apply SMOTE to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# Step 2: Define models with best parameters\n",
        "best_rf = RandomForestClassifier(\n",
        "    n_estimators=300, max_depth=None, min_samples_split=2, random_state=42\n",
        ")\n",
        "\n",
        "best_xgb = XGBClassifier(\n",
        "    n_estimators=300, max_depth=7, learning_rate=0.2, subsample=0.8,\n",
        "    eval_metric='mlogloss', random_state=42\n",
        ")\n",
        "\n",
        "# Add KNN and SVM models\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "svm_model = SVC(probability=True, kernel='rbf', random_state=42)\n",
        "\n",
        "# Step 3: Create the weighted soft voting ensemble\n",
        "ensemble_model_weighted = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', best_rf),   # Random Forest\n",
        "        ('xgb', best_xgb),  # XGBoost\n",
        "        ('knn', knn_model),  # K-Nearest Neighbors\n",
        "        ('svm', svm_model)   # Support Vector Machine\n",
        "    ],\n",
        "    voting='soft',\n",
        "    weights=[2, 3, 1, 1]  # Adjust weights based on performance\n",
        ")\n",
        "\n",
        "# Step 4: Train the ensemble model\n",
        "ensemble_model_weighted.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Step 5: Make predictions on the test data\n",
        "y_pred_weighted = ensemble_model_weighted.predict(X_test_scaled)\n",
        "\n",
        "# Step 6: Evaluate the model's performance\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "classification_report_weighted = classification_report(y_test, y_pred_weighted, target_names=le.classes_)\n",
        "\n",
        "# Output the results\n",
        "print(\"Accuracy with Extended Soft Voting Ensemble:\", accuracy_weighted)\n",
        "print(\"Classification Report:\\n\", classification_report_weighted)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "5H3lN_VxpwE7",
        "ZlolkzGqngcl",
        "imEjdPglqBc8"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}