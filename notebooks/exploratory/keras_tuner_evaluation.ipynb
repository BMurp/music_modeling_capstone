{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of Keras_tuner\n",
    "https://keras.io/keras_tuner/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 13:37:40.935111: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner\n",
    "from tensorflow import keras as K\n",
    "import sys\n",
    "sys.path.insert(0, '../../')\n",
    "from library.notebook_api.data_loader import  ModelDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo in Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "  model = K.Sequential()\n",
    "  model.add(K.layers.Dense(\n",
    "      hp.Choice('units', [8, 16, 32]),\n",
    "      activation='relu'))\n",
    "  model.add(K.layers.Dense(1, activation='relu'))\n",
    "  model.compile(loss='mse')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from ./untitled_project/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo using cnn+ lstm\n",
    "Based on last scenario in CNN_LSTM_MFCC_Classification , define a function for test train data and function for model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#data function\n",
    "def get_numerical_vector_smote_v005_data():\n",
    "    # Initialize ModelDataLoader with the correct directory path\n",
    "    model_data_loader = ModelDataLoader(version='005')\n",
    "    # Reload filtered_df from the original dataset\n",
    "    filtered_df = model_data_loader.df.copy()\n",
    "\n",
    "    # Ensure the features column doesn't have any None values\n",
    "    filtered_df = filtered_df[filtered_df['features'].notnull()]\n",
    "\n",
    "    # Include only relevant genres (ensure 'classical' is included)\n",
    "    selected_genres = ['rock', 'electronic', 'hiphop', 'classical', 'jazz', 'country']\n",
    "\n",
    "    # Filter the DataFrame for the selected genres\n",
    "    filtered_df = filtered_df[filtered_df['label'].isin(selected_genres)].copy()\n",
    "\n",
    "    # Preprocess features to ensure consistent shape\n",
    "    max_length = max(filtered_df['features'].apply(lambda x: len(x)))\n",
    "\n",
    "    # Pad or truncate features\n",
    "    def pad_or_truncate(array, max_length):\n",
    "        if len(array) < max_length:\n",
    "            # Pad with zeros\n",
    "            return np.pad(array, (0, max_length - len(array)), mode='constant')\n",
    "        else:\n",
    "            # Truncate to max_length\n",
    "            return array[:max_length]\n",
    "\n",
    "    # Apply padding/truncating to all features\n",
    "    filtered_df['features_padded'] = filtered_df['features'].apply(lambda x: pad_or_truncate(x, max_length))\n",
    "\n",
    "    # Convert to 2D NumPy array\n",
    "    X = np.array(filtered_df['features_padded'].tolist())\n",
    "\n",
    "    print(f\"Shape of X after padding/truncating: {X.shape}\")\n",
    "\n",
    "    # Extract and encode labels\n",
    "    y = filtered_df['label'].values\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Apply SMOTE for balancing\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y_encoded)\n",
    "\n",
    "    # Verify the balanced class distribution\n",
    "    print(\"Class distribution after SMOTE:\")\n",
    "    unique, counts = np.unique(y_resampled, return_counts=True)\n",
    "    for label, count in zip(label_encoder.inverse_transform(unique), counts):\n",
    "        print(f\"{label}: {count}\")\n",
    "\n",
    "    # Add a channel dimension for CNN input\n",
    "    X_resampled = X_resampled[..., np.newaxis]\n",
    "\n",
    "    print(f\"Shape of X_resampled: {X_resampled.shape}\")\n",
    "    print(f\"Shape of y_resampled: {y_resampled.shape}\")\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "    return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for returning compiled model, passing hp for hyper paramerter tuning \n",
    "def get_cnn_lstm(hp):\n",
    "    #usedict argument for manually set, otherwise use hp\n",
    "    if isinstance(hp, dict):\n",
    "        learning_rate = hp['learning_rate']\n",
    "    else:\n",
    "        learning_rate = hp.Choice(\"learning_rate\", values=[0.001, 0.0001])\n",
    "\n",
    "    # Define the fine-tuned CNN+LSTM model\n",
    "    model = K.models.Sequential([\n",
    "        # CNN Layers\n",
    "        K.layers.Conv1D(128, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        K.layers.BatchNormalization(),\n",
    "        K.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        K.layers.Conv1D(256, kernel_size=3, activation='relu'),\n",
    "        K.layers.BatchNormalization(),\n",
    "        K.layers.MaxPooling1D(pool_size=2),\n",
    "\n",
    "        K.layers.Dropout(0.3),\n",
    "\n",
    "        # LSTM Layers\n",
    "        K.layers.LSTM(256, return_sequences=True, activation='relu'),\n",
    "        K.layers.BatchNormalization(),\n",
    "        K.layers.Dropout(0.4),\n",
    "\n",
    "        K.layers.LSTM(128, return_sequences=False, activation='relu'),\n",
    "        K.layers.Dropout(0.4),\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        K.layers.Dense(256, activation='relu'),\n",
    "        K.layers.Dropout(0.5),\n",
    "\n",
    "        K.layers.Dense(128, activation='relu'),\n",
    "        K.layers.Dropout(0.3),\n",
    "\n",
    "        # Output Layer\n",
    "        K.layers.Dense(6, activation='softmax')  # Adjust for number of classes\n",
    "    ])\n",
    "    # Compile the model with a custom learning rate\n",
    " \n",
    "    optimizer = K.optimizers.Adam(learning_rate=learning_rate)  # Initial learning rate\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@misc{omalley2019kerastuner,\n",
    "    title        = {KerasTuner},\n",
    "    author       = {O'Malley, Tom and Bursztein, Elie and Long, James and Chollet, Fran\\c{c}ois and Jin, Haifeng and Invernizzi, Luca and others},\n",
    "    year         = 2019,\n",
    "    howpublished = {\\url{https://github.com/keras-team/keras-tuner}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X after padding/truncating: (16582, 15)\n",
      "Class distribution after SMOTE:\n",
      "classical: 7261\n",
      "country: 7261\n",
      "electronic: 7261\n",
      "hiphop: 7261\n",
      "jazz: 7261\n",
      "rock: 7261\n",
      "Shape of X_resampled: (43566, 15, 1)\n",
      "Shape of y_resampled: (43566,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_numerical_vector_smote_v005_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 01m 47s]\n",
      "val_accuracy: 0.4779664874076843\n",
      "\n",
      "Best val_accuracy So Far: 0.49839338660240173\n",
      "Total elapsed time: 00h 03m 52s\n"
     ]
    }
   ],
   "source": [
    "# Train \n",
    "#another reference: https://keras.io/keras_tuner/guides/distributed_tuning/\n",
    "# and here as well: https://keras.io/keras_tuner/getting_started/\n",
    "\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    get_cnn_lstm,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=2,\n",
    "    executions_per_trial=1,\n",
    "    overwrite=True,\n",
    "    directory=\"keras_tuner\",\n",
    "    project_name= \"initial_test_01\",\n",
    "    \n",
    "    )\n",
    "\n",
    "tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs = 5,\n",
    "    #steps_per_epoch=600,\n",
    "    validation_data=(X_test, y_test),\n",
    "    #validation_steps=100,\n",
    "    callbacks=[K.callbacks.EarlyStopping(\"val_accuracy\")],\n",
    ")\n",
    "\n",
    "best_model = tuner.get_best_models()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.src.engine.sequential.Sequential"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/273 - 2s - loss: 1.2857 - accuracy: 0.4984 - 2s/epoch - 7ms/step\n",
      "\n",
      "Test Accuracy: 49.84%\n",
      "Test Loss: 1.2857\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#issue with tuner is that by default it is not keeping history, \n",
    "#as workaround can train again to get history of new model \n",
    "#history = tuner.get_best_models()[0].history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras_tuner/initial_test_01\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.0001\n",
      "Score: 0.49839338660240173\n",
      "\n",
      "Trial 1 summary\n",
      "Hyperparameters:\n",
      "learning_rate: 0.001\n",
      "Score: 0.4779664874076843\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1090/1090 [==============================] - 26s 20ms/step - loss: 1.6152 - accuracy: 0.3152 - val_loss: 1.4105 - val_accuracy: 0.4474\n",
      "Epoch 2/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.4746 - accuracy: 0.3961 - val_loss: 1.3585 - val_accuracy: 0.4692\n",
      "Epoch 3/50\n",
      "1090/1090 [==============================] - 24s 22ms/step - loss: 1.4338 - accuracy: 0.4283 - val_loss: 1.3369 - val_accuracy: 0.4812\n",
      "Epoch 4/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.4041 - accuracy: 0.4446 - val_loss: 1.3077 - val_accuracy: 0.4966\n",
      "Epoch 5/50\n",
      "1090/1090 [==============================] - 26s 24ms/step - loss: 1.3769 - accuracy: 0.4627 - val_loss: 1.2596 - val_accuracy: 0.5041\n",
      "Epoch 6/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.3489 - accuracy: 0.4768 - val_loss: 1.2521 - val_accuracy: 0.5170\n",
      "Epoch 7/50\n",
      "1090/1090 [==============================] - 22s 21ms/step - loss: 1.3311 - accuracy: 0.4905 - val_loss: 1.2150 - val_accuracy: 0.5415\n",
      "Epoch 8/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.3146 - accuracy: 0.4964 - val_loss: 1.2104 - val_accuracy: 0.5427\n",
      "Epoch 9/50\n",
      "1090/1090 [==============================] - 21s 19ms/step - loss: 1.2915 - accuracy: 0.5041 - val_loss: 1.1717 - val_accuracy: 0.5550\n",
      "Epoch 10/50\n",
      "1090/1090 [==============================] - 19s 17ms/step - loss: 1.2782 - accuracy: 0.5099 - val_loss: 1.1947 - val_accuracy: 0.5410\n",
      "Epoch 11/50\n",
      "1090/1090 [==============================] - 27s 25ms/step - loss: 1.2675 - accuracy: 0.5153 - val_loss: 1.1884 - val_accuracy: 0.5391\n",
      "Epoch 12/50\n",
      "1090/1090 [==============================] - 27s 24ms/step - loss: 1.2644 - accuracy: 0.5203 - val_loss: 1.1763 - val_accuracy: 0.5545\n",
      "Epoch 13/50\n",
      "1090/1090 [==============================] - 24s 22ms/step - loss: 1.2453 - accuracy: 0.5258 - val_loss: 1.1381 - val_accuracy: 0.5613\n",
      "Epoch 14/50\n",
      "1090/1090 [==============================] - 20s 18ms/step - loss: 1.2363 - accuracy: 0.5297 - val_loss: 1.1812 - val_accuracy: 0.5504\n",
      "Epoch 15/50\n",
      "1090/1090 [==============================] - 28s 26ms/step - loss: 1.2263 - accuracy: 0.5357 - val_loss: 1.1516 - val_accuracy: 0.5637\n",
      "Epoch 16/50\n",
      "1090/1090 [==============================] - 24s 22ms/step - loss: 1.2164 - accuracy: 0.5375 - val_loss: 1.1210 - val_accuracy: 0.5771\n",
      "Epoch 17/50\n",
      "1090/1090 [==============================] - 25s 23ms/step - loss: 1.2147 - accuracy: 0.5387 - val_loss: 1.1664 - val_accuracy: 0.5547\n",
      "Epoch 18/50\n",
      "1090/1090 [==============================] - 23s 22ms/step - loss: 1.2049 - accuracy: 0.5424 - val_loss: 1.1563 - val_accuracy: 0.5616\n",
      "Epoch 19/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.1926 - accuracy: 0.5481 - val_loss: 1.1089 - val_accuracy: 0.5849\n",
      "Epoch 20/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.1852 - accuracy: 0.5531 - val_loss: 1.0977 - val_accuracy: 0.5861\n",
      "Epoch 21/50\n",
      "1090/1090 [==============================] - 22s 20ms/step - loss: 1.1744 - accuracy: 0.5556 - val_loss: 1.0761 - val_accuracy: 0.5922\n",
      "Epoch 22/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.1736 - accuracy: 0.5571 - val_loss: 1.0851 - val_accuracy: 0.5935\n",
      "Epoch 23/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.1597 - accuracy: 0.5586 - val_loss: 1.0946 - val_accuracy: 0.5815\n",
      "Epoch 24/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.1461 - accuracy: 0.5654 - val_loss: 1.0686 - val_accuracy: 0.5923\n",
      "Epoch 25/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.1475 - accuracy: 0.5626 - val_loss: 1.0253 - val_accuracy: 0.6146\n",
      "Epoch 26/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.1313 - accuracy: 0.5732 - val_loss: 1.0266 - val_accuracy: 0.6243\n",
      "Epoch 27/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.1285 - accuracy: 0.5728 - val_loss: 1.0582 - val_accuracy: 0.5946\n",
      "Epoch 28/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.1133 - accuracy: 0.5787 - val_loss: 1.0550 - val_accuracy: 0.6044\n",
      "Epoch 29/50\n",
      "1090/1090 [==============================] - 23s 21ms/step - loss: 1.1095 - accuracy: 0.5807 - val_loss: 1.0241 - val_accuracy: 0.6091\n",
      "Epoch 30/50\n",
      "1090/1090 [==============================] - 26s 24ms/step - loss: 1.1015 - accuracy: 0.5849 - val_loss: 1.0136 - val_accuracy: 0.6145\n",
      "Epoch 31/50\n",
      "1090/1090 [==============================] - 21s 19ms/step - loss: 1.0871 - accuracy: 0.5887 - val_loss: 0.9942 - val_accuracy: 0.6220\n",
      "Epoch 32/50\n",
      "1090/1090 [==============================] - 21s 19ms/step - loss: 1.0851 - accuracy: 0.5903 - val_loss: 0.9775 - val_accuracy: 0.6378\n",
      "Epoch 33/50\n",
      "1090/1090 [==============================] - 20s 19ms/step - loss: 1.0814 - accuracy: 0.5928 - val_loss: 1.0171 - val_accuracy: 0.6149\n",
      "Epoch 34/50\n",
      "1090/1090 [==============================] - 21s 19ms/step - loss: 1.0814 - accuracy: 0.5892 - val_loss: 0.9625 - val_accuracy: 0.6445\n",
      "Epoch 35/50\n",
      "1090/1090 [==============================] - 24s 22ms/step - loss: 1.0707 - accuracy: 0.5957 - val_loss: 1.0549 - val_accuracy: 0.5832\n",
      "Epoch 36/50\n",
      "1090/1090 [==============================] - 32s 30ms/step - loss: 1.0614 - accuracy: 0.5973 - val_loss: 0.9357 - val_accuracy: 0.6468\n",
      "Epoch 37/50\n",
      "1090/1090 [==============================] - 29s 27ms/step - loss: 1.0549 - accuracy: 0.6013 - val_loss: 0.9671 - val_accuracy: 0.6346\n",
      "Epoch 38/50\n",
      "1090/1090 [==============================] - 28s 26ms/step - loss: 1.0494 - accuracy: 0.6052 - val_loss: 0.9553 - val_accuracy: 0.6361\n",
      "Epoch 39/50\n",
      "1090/1090 [==============================] - 27s 25ms/step - loss: 1.0438 - accuracy: 0.6049 - val_loss: 0.9182 - val_accuracy: 0.6602\n",
      "Epoch 40/50\n",
      "1090/1090 [==============================] - 26s 23ms/step - loss: 1.0310 - accuracy: 0.6100 - val_loss: 0.9270 - val_accuracy: 0.6469\n",
      "Epoch 41/50\n",
      "1090/1090 [==============================] - 28s 26ms/step - loss: 1.0336 - accuracy: 0.6079 - val_loss: 0.9290 - val_accuracy: 0.6451\n",
      "Epoch 42/50\n",
      "1090/1090 [==============================] - 32s 29ms/step - loss: 1.0230 - accuracy: 0.6108 - val_loss: 0.9145 - val_accuracy: 0.6583\n",
      "Epoch 43/50\n",
      "1090/1090 [==============================] - 28s 25ms/step - loss: 1.0234 - accuracy: 0.6180 - val_loss: 0.9571 - val_accuracy: 0.6375\n",
      "Epoch 44/50\n",
      "1090/1090 [==============================] - 24s 22ms/step - loss: 1.0220 - accuracy: 0.6114 - val_loss: 0.9158 - val_accuracy: 0.6525\n",
      "Epoch 45/50\n",
      "1090/1090 [==============================] - 19s 17ms/step - loss: 1.0044 - accuracy: 0.6210 - val_loss: 0.9758 - val_accuracy: 0.6215\n",
      "Epoch 46/50\n",
      "1090/1090 [==============================] - 20s 18ms/step - loss: 1.0144 - accuracy: 0.6180 - val_loss: 0.9197 - val_accuracy: 0.6564\n",
      "Epoch 47/50\n",
      "1090/1090 [==============================] - 22s 20ms/step - loss: 0.9970 - accuracy: 0.6214 - val_loss: 0.9132 - val_accuracy: 0.6518\n",
      "Epoch 48/50\n",
      "1090/1090 [==============================] - 19s 17ms/step - loss: 0.9952 - accuracy: 0.6258 - val_loss: 0.8964 - val_accuracy: 0.6716\n",
      "Epoch 49/50\n",
      "1090/1090 [==============================] - 20s 18ms/step - loss: 0.9862 - accuracy: 0.6283 - val_loss: 0.8924 - val_accuracy: 0.6704\n",
      "Epoch 50/50\n",
      "1090/1090 [==============================] - 20s 18ms/step - loss: 0.9851 - accuracy: 0.6284 - val_loss: 0.8851 - val_accuracy: 0.6681\n"
     ]
    }
   ],
   "source": [
    "selected_hp = {'learning_rate':  0.0001}\n",
    "type(selected_hp)\n",
    "history = get_cnn_lstm(selected_hp).fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        K.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        #ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "        #lr_scheduler  # Learning rate scheduler\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [1.5168441534042358], 'accuracy': [0.37828531861305237], 'val_loss': [1.4481579065322876], 'val_accuracy': [0.3932752013206482]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
